{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TJ3Ue1Po8NpB"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras import layers\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import tifffile as tiff\n",
        "import os\n",
        "import cv2\n",
        "from skimage.transform import resize\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import os\n",
        "from PIL import Image\n",
        "from keras.layers import Dense,Flatten,Dropout\n",
        "from keras.callbacks import EarlyStopping\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "from keras import regularizers\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.applications import EfficientNetB0\n",
        "from keras.models import Sequential\n",
        "from keras.applications.vgg16 import preprocess_input\n",
        "from keras import layers, models\n",
        "from keras.applications.resnet50 import ResNet50"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "4wz0HHLZ8aY9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20137565-83fc-417e-aeea-1a86966a038d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "img_width=224\n",
        "img_height=224\n",
        "img_size=128\n",
        "epochs=50\n",
        "NUM_CLASSES=2"
      ],
      "metadata": {
        "id": "LsjLogyW8crr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_generator = ImageDataGenerator(\n",
        "    rescale = 1. / 255,\n",
        "    shear_range = 0.2,\n",
        "    zoom_range = 0.2,\n",
        "    horizontal_flip = True,\n",
        "    vertical_flip = True,\n",
        "    rotation_range = 180,\n",
        "    width_shift_range = 0.2,\n",
        "    height_shift_range = 0.2,\n",
        "    validation_split = 0.2)\n",
        "\n",
        "train_generator = data_generator.flow_from_directory(\n",
        "    '/content/drive/MyDrive/deep learning/train',\n",
        "    target_size =(img_width, img_height),\n",
        "    batch_size = batch_size,\n",
        "    shuffle = True,\n",
        "    #class_mode = 'binary',\n",
        "    seed = 42,\n",
        "    subset='training')\n",
        "\n",
        "validation_generator = data_generator.flow_from_directory(\n",
        "    '/content/drive/MyDrive/deep learning/test',\n",
        "    target_size =(img_width, img_height),\n",
        "    batch_size = batch_size,\n",
        "    shuffle = True,\n",
        "   # class_mode = 'binary',\n",
        "    seed = 42,\n",
        "    subset='validation')"
      ],
      "metadata": {
        "id": "ItJ9BI4k8lGN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "157159c2-3e41-43ed-a34a-15d17a274d9c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 84 images belonging to 2 classes.\n",
            "Found 4 images belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tqdm\n",
        "train_generator.reset()\n",
        "X_train, y_train = next(train_generator)\n",
        "for i in tqdm.tqdm(range(int(train_generator.n/batch_size)-1)):\n",
        "  img, label = next(train_generator)\n",
        "  X_train = np.append(X_train, img, axis=0 )\n",
        "  y_train = np.append(y_train, label, axis=0)\n",
        "print(X_train.shape, y_train.shape)"
      ],
      "metadata": {
        "id": "Z_1QYgqq8pcq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test, y_test = next(validation_generator)\n",
        "for i in tqdm.tqdm(range(int(train_generator.n/batch_size)-1)):\n",
        "  img, label = next(train_generator)\n",
        "  X_train = np.append(X_train, img, axis=0 )\n",
        "  y_train = np.append(y_train, label, axis=0)\n",
        "print(X_train.shape, y_train.shape)"
      ],
      "metadata": {
        "id": "n8iBcM4b8x-T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Create an ImageDataGenerator with desired augmentation parameters\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=30,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "# Function to apply augmentation twice to each dataset\n",
        "def augment_dataset(dataset):\n",
        "    augmented_data = []\n",
        "    for img in dataset:\n",
        "        img = np.expand_dims(img, axis=0)  # Expanding dimensions for batch size\n",
        "        for _ in range(1):  # Apply augmentation twice\n",
        "            for batch in datagen.flow(img, batch_size=1):\n",
        "                augmented_data.append(np.squeeze(batch))  # Remove the batch dimension\n",
        "                break  # Stop the loop after one augmented image\n",
        "    return np.array(augmented_data)"
      ],
      "metadata": {
        "id": "XsTVU6wG83bs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply augmentation twice to X_train, X_test, and X_val\n",
        "X_train_augmented = augment_dataset(X_train)\n",
        "X_test_augmented = augment_dataset(X_test)\n",
        "#X_val_augmented = augment_dataset(X_val)\n",
        "y_train_augmented = np.repeat(y_train, 1, axis=0)  # Adjust based on augmentation factor\n",
        "y_test_augmented = np.repeat(y_test, 1, axis=0)  # Adjust based on augmentation factor"
      ],
      "metadata": {
        "id": "WS8rc4D186KM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.utils import shuffle\n",
        "\n",
        "# Combine original data with augmented data\n",
        "X_train_combined = np.concatenate((X_train, X_train_augmented))\n",
        "y_train_combined = np.concatenate((y_train, y_train_augmented))\n",
        "\n",
        "X_test_combined = np.concatenate((X_test, X_test_augmented))\n",
        "y_test_combined = np.concatenate((y_test, y_test_augmented))\n",
        "\n",
        "\n",
        "# Shuffle the combined data\n",
        "X_train_combined, y_train_combined = shuffle(X_train_combined, y_train_combined, random_state=42)\n",
        "X_test_combined, y_test_combined = shuffle(X_test_combined, y_test_combined, random_state=42)"
      ],
      "metadata": {
        "id": "BdkFCBv5890z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.applications import vgg19\n",
        "\n",
        "\n",
        "img_rows, img_cols = 224, 224\n",
        "\n",
        "resnet50_base = ResNet50(weights='imagenet', include_top=False, input_shape=(img_rows, img_cols, 3))\n",
        "\n",
        "\n",
        "for layer in resnet50_base.layers:\n",
        "    layer.trainable = False\n",
        "# Let's print our layers\n",
        "for (i,layer) in enumerate(resnet50_base.layers):\n",
        "    print(str(i) + \" \"+ layer.__class__.__name__, layer.trainable)"
      ],
      "metadata": {
        "id": "f-sO0Gv49Atl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten, GlobalAveragePooling2D\n",
        "from keras.layers import Conv2D, MaxPooling2D, ZeroPadding2D\n",
        "\n",
        "from keras.models import Model\n",
        "def lw(bottom_model, num_classes):\n",
        "    \"\"\"creates the top or head of the model that will be\n",
        "    placed ontop of the bottom layers\"\"\"\n",
        "\n",
        "    top_model = bottom_model.output\n",
        "    top_model = GlobalAveragePooling2D()(top_model)\n",
        "    top_model = Dense(1024,activation='relu')(top_model)\n",
        "    top_model = Dropout(0.1)(top_model)\n",
        "    top_model = Dense(1024,activation='relu')(top_model)\n",
        "    top_model = Dropout(0.1)(top_model)\n",
        "    top_model = Dense(1024,activation='relu')(top_model)\n",
        "    top_model = Dropout(0.1)(top_model)\n",
        "    top_model = Dense(num_classes,activation='sigmoid')(top_model)\n",
        "    return top_model"
      ],
      "metadata": {
        "id": "c3f-QlTK9F7T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten, GlobalAveragePooling2D\n",
        "from keras.layers import Conv2D, MaxPooling2D, ZeroPadding2D\n",
        "\n",
        "from keras.models import Model\n",
        "\n",
        "\n",
        "num_classes = 2\n",
        "\n",
        "FC_Head = lw(resnet50_base, num_classes)\n",
        "\n",
        "\n",
        "model = Model(inputs = resnet50_base.input, outputs = FC_Head)"
      ],
      "metadata": {
        "id": "85zzSoPw9ImL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Model\n",
        "import tensorflow as tf\n",
        "from keras import backend as K\n",
        "metrics = [\n",
        "        'accuracy',\n",
        "        tf.keras.metrics.AUC(),\n",
        "        tf.keras.metrics.Recall(),\n",
        "        tf.keras.metrics.Precision(),\n",
        "        tf.keras.metrics.SpecificityAtSensitivity(0.5),\n",
        "        tf.keras.metrics.SensitivityAtSpecificity(0.5),\n",
        "        tf.keras.metrics.FalseNegatives(),\n",
        "        tf.keras.metrics.FalsePositives(),\n",
        "        tf.keras.metrics.TrueNegatives(),\n",
        "        tf.keras.metrics.TruePositives(),]"
      ],
      "metadata": {
        "id": "LH3FLzd39LvU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "callbacks = [\n",
        "    tf.keras.callbacks.EarlyStopping(\n",
        "    monitor='val_loss',patience=3,\n",
        "         restore_best_weights=True,\n",
        "    )\n",
        "]\n",
        "history = model.fit(X_train_combined, y_train_combined, epochs=50,batch_size=32,validation_split=0.2,callbacks=callbacks)"
      ],
      "metadata": {
        "id": "8lpKtfPJ9PHz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(X_test,y_test)"
      ],
      "metadata": {
        "id": "Ssv0zrK39Rws"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "acc = history.history['accuracy']\n",
        "val_acc= history.history['val_accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(len(acc))\n",
        "\n",
        "plt.plot(epochs,acc,'r',label=\"Training accuracy\")\n",
        "plt.plot(epochs,val_acc,'b',label=\"Validation accuracy\")\n",
        "plt.title('Training and Validation accuracy')\n",
        "plt.legend(loc=0)\n",
        "plt.figure()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "wTSc-BuN9U00"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(epochs,loss,'r',label=\"Training loss\")\n",
        "plt.plot(epochs,val_loss,'b',label=\"Validation loss\")\n",
        "plt.title('Training and Validation loss')\n",
        "plt.legend(loc=0)\n",
        "plt.figure()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "XqsGNoaK9ctN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}